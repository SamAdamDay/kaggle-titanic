{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import DataHandlerTitantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and split into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh = DataHandlerTitantic(34545234)\n",
    "dh.load_data(\"data/train.csv\", \"data/test.csv\")\n",
    "dh.shuffle_split(0.9)\n",
    "dh.full_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use socio-economic class, gender, age, fare, port of embark, number of siblings/spouses aboard and number of parents/childern aboard as features. Let's impute the missing values, and do a little preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>IsFemale</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>764</td>\n",
       "      <td>1</td>\n",
       "      <td>Carter, Mrs. William Ernest (Lucile Polk)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113760</td>\n",
       "      <td>120.000</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>Stahelin-Maeglin, Dr. Max</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13214</td>\n",
       "      <td>30.500</td>\n",
       "      <td>B50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>Collyer, Miss. Marjorie \"Lottie\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C.A. 31921</td>\n",
       "      <td>26.250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>Windelov, Mr. Einar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SOTON/OQ 3101317</td>\n",
       "      <td>7.250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>591</td>\n",
       "      <td>0</td>\n",
       "      <td>Rintamaki, Mr. Matti</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>STON/O 2. 3101273</td>\n",
       "      <td>7.125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived                                       Name  \\\n",
       "763          764         1  Carter, Mrs. William Ernest (Lucile Polk)   \n",
       "632          633         1                  Stahelin-Maeglin, Dr. Max   \n",
       "237          238         1           Collyer, Miss. Marjorie \"Lottie\"   \n",
       "491          492         0                        Windelov, Mr. Einar   \n",
       "590          591         0                       Rintamaki, Mr. Matti   \n",
       "\n",
       "     IsFemale   Age  SibSp  Parch             Ticket     Fare    Cabin  \\\n",
       "763       1.0  36.0    1.0    2.0             113760  120.000  B96 B98   \n",
       "632       0.0  32.0    0.0    0.0              13214   30.500      B50   \n",
       "237       1.0   8.0    0.0    2.0         C.A. 31921   26.250      NaN   \n",
       "491       0.0  21.0    0.0    0.0   SOTON/OQ 3101317    7.250      NaN   \n",
       "590       0.0  35.0    0.0    0.0  STON/O 2. 3101273    7.125      NaN   \n",
       "\n",
       "     Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  \n",
       "763       1.0       0.0       0.0         0.0         0.0         1.0  \n",
       "632       1.0       0.0       0.0         1.0         0.0         0.0  \n",
       "237       0.0       1.0       0.0         0.0         0.0         1.0  \n",
       "491       0.0       0.0       1.0         0.0         0.0         1.0  \n",
       "590       0.0       0.0       1.0         0.0         0.0         1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impute_columns = [\n",
    "    \"Pclass_1\",\n",
    "    \"Pclass_2\",\n",
    "    \"Pclass_3\",\n",
    "    \"IsFemale\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked_C\",\n",
    "    \"Embarked_Q\",\n",
    "    \"Embarked_S\"\n",
    "]\n",
    "\n",
    "dh1 = dh.to_is_female()\\\n",
    "        .make_dummies([\"Pclass\", \"Embarked\"])\\\n",
    "        .impute_values(impute_columns, strategy=\"knn\")\n",
    "dh1.train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up the data sets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "feature_columns1 = [\n",
    "    \"Pclass_1\",\n",
    "    \"Pclass_2\",\n",
    "    \"Pclass_3\",\n",
    "    \"IsFemale\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked_C\",\n",
    "    \"Embarked_Q\",\n",
    "    \"Embarked_S\"\n",
    "]\n",
    "\n",
    "train_dataset1 = dh1.get_train_pytorch_dataset(feature_columns1)\n",
    "eval_dataset1 = dh1.get_eval_pytorch_dataset(feature_columns1)\n",
    "\n",
    "train_dataloader1 = DataLoader(train_dataset1, batch_size=batch_size)\n",
    "eval_dataloader1 = DataLoader(eval_dataset1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model. We'll use a hidden layer with 6 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=6, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(11, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "model1 = Model1().to(device)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross entropy loss and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn1 = nn.CrossEntropyLoss()\n",
    "optimiser1 = torch.optim.SGD(model1.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the generic training loop, which we'll use for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimiser):\n",
    "    \n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # The number of datapoints\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # Loop over each batch of datapoints\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # Make sure the data is on the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Make a prediction using the current parameters\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute the loss for this prediction\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Set all the gradients to zero\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Propagate the loss backwards to compute the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Do one step of optimisation\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Print the loss function and train error every so often\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss_val = loss.item()\n",
    "        #     train_err = 1 - (pred.argmax(1) == y).count_nonzero() / len(y)\n",
    "        #     pos = batch * len(X)\n",
    "        #     print(f\"loss: {loss_val:>7f}, train err: {train_err:09.5%} \"\n",
    "        #          f\"[{pos:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function outputs the error for either the train set or the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error(dataloader, dataloader_name, model):\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the number of datapoints\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # The number of correct predictions\n",
    "    correct = 0\n",
    "\n",
    "    # We don't want to be computing the gradients\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Loop through the minibatches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # Make sure the data is on the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute the model predictions\n",
    "            pred = model(X)\n",
    "\n",
    "            # Update with the number of correct predictions\n",
    "            correct += (pred.argmax(1) == y).count_nonzero()\n",
    "\n",
    "    # Compute the error for the whole dataset\n",
    "    error = 1 - correct / len(dataloader.dataset)\n",
    "\n",
    "    # Output it\n",
    "    print(f\"{dataloader_name.capitalize()} error: {error:09.5%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000\n",
      "----------------------------------\n",
      "Train error: 22.09738%\n",
      "Validation error: 20.00000%\n",
      "\n",
      "Epoch 2000\n",
      "----------------------------------\n",
      "Train error: 19.85019%\n",
      "Validation error: 17.77778%\n",
      "\n",
      "Epoch 3000\n",
      "----------------------------------\n",
      "Train error: 19.97503%\n",
      "Validation error: 17.77778%\n",
      "\n",
      "Epoch 4000\n",
      "----------------------------------\n",
      "Train error: 19.47566%\n",
      "Validation error: 17.77778%\n",
      "\n",
      "Epoch 5000\n",
      "----------------------------------\n",
      "Train error: 18.72659%\n",
      "Validation error: 16.66667%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "for t in range(1, epochs+1):\n",
    "    train(train_dataloader1, model1, loss_fn1, optimiser1)\n",
    "    if t % 1000 == 0:\n",
    "        print(f\"Epoch {t}\")\n",
    "        print(\"----------------------------------\")\n",
    "        print_error(train_dataloader1, \"train\", model1)\n",
    "        print_error(eval_dataloader1, \"validation\", model1)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slower learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a slower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000\n",
      "----------------------------------\n",
      "Train error: 20.84894%\n",
      "Validation error: 21.11111%\n",
      "\n",
      "Epoch 2000\n",
      "----------------------------------\n",
      "Train error: 20.09987%\n",
      "Validation error: 16.66667%\n",
      "\n",
      "Epoch 3000\n",
      "----------------------------------\n",
      "Train error: 19.47566%\n",
      "Validation error: 15.55555%\n",
      "\n",
      "Epoch 4000\n",
      "----------------------------------\n",
      "Train error: 19.47566%\n",
      "Validation error: 15.55555%\n",
      "\n",
      "Epoch 5000\n",
      "----------------------------------\n",
      "Train error: 19.10113%\n",
      "Validation error: 15.55555%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model2 = Model1().to(device)\n",
    "\n",
    "# Use the same data handler\n",
    "dh2 = dh1\n",
    "\n",
    "# Use the same data sets\n",
    "train_dataset2 = train_dataset1\n",
    "eval_dataset2 = eval_dataset1\n",
    "\n",
    "# Define new data loaders\n",
    "train_dataloader2 = DataLoader(train_dataset2, batch_size=batch_size)\n",
    "eval_dataloader2 = DataLoader(eval_dataset2, batch_size=batch_size)\n",
    "\n",
    "# Use the same loss function, and SGD with a lower learning rate\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimiser2 = torch.optim.SGD(model2.parameters(), lr=1e-2)\n",
    "\n",
    "# Train the model\n",
    "epochs = 5000\n",
    "for t in range(1, epochs+1):\n",
    "    train(train_dataloader2, model2, loss_fn2, optimiser2)\n",
    "    if t % 1000 == 0:\n",
    "        print(f\"Epoch {t}\")\n",
    "        print(\"----------------------------------\")\n",
    "        print_error(train_dataloader2, \"train\", model2)\n",
    "        print_error(eval_dataloader2, \"validation\", model2)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Adam optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's try using the Adam optimiser instead of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000\n",
      "----------------------------------\n",
      "Train error: 17.72784%\n",
      "Validation error: 16.66667%\n",
      "\n",
      "Epoch 2000\n",
      "----------------------------------\n",
      "Train error: 17.10362%\n",
      "Validation error: 16.66667%\n",
      "\n",
      "Epoch 3000\n",
      "----------------------------------\n",
      "Train error: 17.22847%\n",
      "Validation error: 16.66667%\n",
      "\n",
      "Epoch 4000\n",
      "----------------------------------\n",
      "Train error: 16.97878%\n",
      "Validation error: 16.66667%\n",
      "\n",
      "Epoch 5000\n",
      "----------------------------------\n",
      "Train error: 16.85393%\n",
      "Validation error: 16.66667%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model3 = Model1().to(device)\n",
    "\n",
    "# Use the same data handler\n",
    "dh3 = dh1\n",
    "\n",
    "# Use the same data sets\n",
    "train_dataset3 = train_dataset1\n",
    "eval_dataset3 = eval_dataset1\n",
    "\n",
    "# Define new data loaders\n",
    "train_dataloader3 = DataLoader(train_dataset3, batch_size=batch_size)\n",
    "eval_dataloader3 = DataLoader(eval_dataset3, batch_size=batch_size)\n",
    "\n",
    "# Use the same loss function, and the Adam optimiser\n",
    "loss_fn3 = nn.CrossEntropyLoss()\n",
    "optimiser3 = torch.optim.Adam(model3.parameters())\n",
    "\n",
    "# Train the model\n",
    "epochs = 5000\n",
    "for t in range(1, epochs+1):\n",
    "    train(train_dataloader3, model3, loss_fn3, optimiser3)\n",
    "    if t % 1000 == 0:\n",
    "        print(f\"Epoch {t}\")\n",
    "        print(\"----------------------------------\")\n",
    "        print_error(train_dataloader3, \"train\", model3)\n",
    "        print_error(eval_dataloader3, \"validation\", model3)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using a network with two hidden layers, first increasing dimension, then decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model4(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(11, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "model4 = Model4().to(device)\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000\n",
      "----------------------------------\n",
      "Train error: 30.21224%\n",
      "Validation error: 32.22222%\n",
      "\n",
      "Epoch 2000\n",
      "----------------------------------\n",
      "Train error: 25.34332%\n",
      "Validation error: 24.44444%\n",
      "\n",
      "Epoch 3000\n",
      "----------------------------------\n",
      "Train error: 18.72659%\n",
      "Validation error: 13.33333%\n",
      "\n",
      "Epoch 4000\n",
      "----------------------------------\n",
      "Train error: 17.97753%\n",
      "Validation error: 15.55555%\n",
      "\n",
      "Epoch 5000\n",
      "----------------------------------\n",
      "Train error: 18.35206%\n",
      "Validation error: 16.66667%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the same data handler\n",
    "dh4 = dh1\n",
    "\n",
    "# Use the same data sets\n",
    "train_dataset4 = train_dataset1\n",
    "eval_dataset4 = eval_dataset1\n",
    "\n",
    "# Define new data loaders\n",
    "train_dataloader4 = DataLoader(train_dataset4, batch_size=batch_size)\n",
    "eval_dataloader4 = DataLoader(eval_dataset4, batch_size=batch_size)\n",
    "\n",
    "# Use the cross entropy loss function, and SGD\n",
    "loss_fn4 = nn.CrossEntropyLoss()\n",
    "optimiser4 = torch.optim.SGD(model4.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "epochs = 5000\n",
    "for t in range(1, epochs+1):\n",
    "    train(train_dataloader4, model4, loss_fn4, optimiser4)\n",
    "    if t % 1000 == 0:\n",
    "        print(f\"Epoch {t}\")\n",
    "        print(\"----------------------------------\")\n",
    "        print_error(train_dataloader4, \"train\", model4)\n",
    "        print_error(eval_dataloader4, \"validation\", model4)\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
